{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PART 1 - BENCHMARKING ANALYSIS",
   "id": "509fddbf804d00b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the first part of the project, we are going to load a pretrained model from HuggingFace, the CyberPeace-Institute/SecureBERT-NER, load a dataset, the DNRTI, and evaluate the model using the dataset\n",
    "\n",
    "Let's start with imports"
   ],
   "id": "d5f3316e066a3337"
  },
  {
   "cell_type": "code",
   "id": "9c8c43263fd9babd",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.248172Z",
     "start_time": "2024-10-16T07:09:03.238365Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import time\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the model and tokenizer loading function",
   "id": "2f5d3fc452d35b66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.370562Z",
     "start_time": "2024-10-16T07:09:03.367342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pretrained_ner_model(model_name=\"CyberPeace-Institute/SecureBERT-NER\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ],
   "id": "d307da0ef2469535",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining dataset loading function",
   "id": "ec6f9b108ef5da1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.376542Z",
     "start_time": "2024-10-16T07:09:03.372056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_dataset(file_path, delimiter=\" \"):\n",
    "    \"\"\"\n",
    "    Load a dataset\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                    current_sentence = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                splits = line.strip().split(delimiter)\n",
    "                if len(splits) >= 2:\n",
    "                    token, label = splits[0], splits[-1]\n",
    "                    current_sentence.append(token)\n",
    "                    current_labels.append(label)\n",
    "        # Add the last sentence - it doesn't have newline after\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "    \n",
    "    return sentences, labels"
   ],
   "id": "a30edd2f5dd0e048",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because the model and the dataset have different tags/labels, we map the dataset tag to the model tag by creating a dictionary. The dataset's OffAct and Way tags are arbitrarily chosen to be the model's ACT, and the dataset's Exp tag is chosen to be the model's VULID tag. If there is no corresponding tag, the default value will be 'O'",
   "id": "d03a042569c3afd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.382959Z",
     "start_time": "2024-10-16T07:09:03.378137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tag_mapping = {\n",
    "    # Dataset tag : Model tag\n",
    "    \"HackOrg\": \"APT\",\n",
    "    \"SecTeam\": \"SECTEAM\",\n",
    "    \"Idus\": \"IDTY\",\n",
    "    \"Org\": \"IDTY\",\n",
    "    \"OffAct\": \"ACT\", # duplicate arbitrary choice\n",
    "    \"Way\": \"ACT\", # duplicate arbitrary choice\n",
    "    \"Exp\": \"VULID\", # duplicate arbitrary choice\n",
    "    \"Tool\": \"MAL\",\n",
    "    \"SamFile\": \"FILE\",\n",
    "    \"Time\": \"TIME\",\n",
    "    \"Area\": \"LOC\"\n",
    "}\n",
    "\n",
    "def map_tags(labels, tag_mapping, default_tag=\"O\"):\n",
    "    \"\"\"\n",
    "    Map the dataset's tags to the pre-trained model's tags based on the mapping.\n",
    "    Unmapped tags are set to the default_tag.\n",
    "    \"\"\"\n",
    "    mapped_labels = []\n",
    "    for label_seq in labels:\n",
    "        mapped_seq = []\n",
    "        for i, label in enumerate(label_seq):\n",
    "            mapped_label = tag_mapping.get(label[2:], default_tag)\n",
    "            if mapped_label != default_tag:\n",
    "                mapped_label = f\"{label[:2]}{mapped_label}\"\n",
    "            mapped_seq.append(mapped_label)\n",
    "        mapped_labels.append(mapped_seq)\n",
    "    return mapped_labels\n"
   ],
   "id": "a811c1270eb94b7",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now preprocess, tokenize the data, and align it",
   "id": "2a5da33c1e94daf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.387805Z",
     "start_time": "2024-10-16T07:09:03.383788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(tokenizer, sentences, mapped_labels, label_list, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize the dataset and align labels with tokens.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(mapped_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens or padding\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Start of a new word, assign label\n",
    "                label_ids.append(label_to_id.get(label[word_idx], -100))\n",
    "            else:\n",
    "                # Same word, assign -100 - subword tokens ignored\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs"
   ],
   "id": "a3761cffe59ee17c",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the project's definition, we stated that the model's ACT, OS and TOOL tags are all valid predictions for the OffAct and Way dataset tags, and VULID and VULNAME are valid predictions for Exp dataset tag. Because of that, we define that if the model predicted for example a TOOL or a OS tag, it is equivalent that it predicted a ACT tag. And because above we arbitrarily chose the dataset-model mapping to be ACT, we also convert TOOL and OS predictions to be ACT - as it is a correct prediction. The same holds true for VULNAME, we map it to VULID.\n",
    "Also, by definition, the are some mapped classes in the model that are not mapped in our dataset, so we will want to ignore those predictions, so we define those classes as mapping to 'O'."
   ],
   "id": "70e7b6910ba63557"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.392634Z",
     "start_time": "2024-10-16T07:09:03.389092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tag_mapping_correction = {\n",
    "    # Model tag : Model tag\n",
    "    \"TOOL\": \"ACT\",\n",
    "    \"OS\": \"ACT\",\n",
    "    \"VULNAME\": \"VULID\",\n",
    "    \"DOM\": \"O\",\n",
    "    \"ENCR\": \"O\",\n",
    "    \"IP\": \"O\",\n",
    "    \"URL\": \"O\",\n",
    "    \"MD5\": \"O\",\n",
    "    \"PROT\": \"O\",\n",
    "    \"EMAIL\": \"O\",\n",
    "    \"SHA1\": \"O\",\n",
    "    \"SHA2\": \"O\"\n",
    "}\n",
    "\n",
    "def fix_unmapped_model_labels(pred_label: str) -> str:\n",
    "    prefix = pred_label[:2]\n",
    "    postfix = pred_label[2:]\n",
    "    fixed_label = tag_mapping_correction.get(postfix, postfix)\n",
    "    if fixed_label != \"O\":\n",
    "        fixed_label = f\"{prefix}{fixed_label}\"\n",
    "    return fixed_label\n",
    "\n",
    "def align_predictions(predictions, labels, label_list):\n",
    "    \"\"\"\n",
    "    Align the predictions with the labels, ignoring special tokens.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for i, label in enumerate(labels):\n",
    "        current_true = []\n",
    "        current_pred = []\n",
    "        for j, lab in enumerate(label):\n",
    "            if lab != -100:\n",
    "                current_true.append(label_list[lab])\n",
    "                pred_label = label_list[predictions[i][j]]\n",
    "                current_pred.append(fix_unmapped_model_labels(pred_label))\n",
    "        true_labels.append(current_true)\n",
    "        true_preds.append(current_pred)\n",
    "    \n",
    "    return true_labels, true_preds"
   ],
   "id": "65ebf4db4eb7a5b9",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Torch dataset wrapper",
   "id": "63bcd4a82d02158f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.396431Z",
     "start_time": "2024-10-16T07:09:03.393432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ],
   "id": "73104217d056701b",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we define an evaluation function that evaluates the model and outputs metrics.",
   "id": "f9d68fb03033ff8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:03.405401Z",
     "start_time": "2024-10-16T07:09:03.397149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, tokenized_data, label_list, batch_size=32):\n",
    "    \"\"\"\n",
    "    Run the model on the dataset and evaluate.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    dataset = NERDataset(tokenized_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # Measure start time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_length, num_labels)\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Measure end time\n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            all_predictions.append(preds)\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    \n",
    "    # Align predictions and labels\n",
    "    true_labels, true_preds = align_predictions(all_predictions, all_labels, label_list)\n",
    "    \n",
    "    # Calculate average latency\n",
    "    average_latency = np.mean(latencies)\n",
    "    \n",
    "    # Calculate metrics using seqeval\n",
    "    precision = precision_score(true_labels, true_preds, average='macro')\n",
    "    recall = recall_score(true_labels, true_preds, average='macro')\n",
    "    f1 = f1_score(true_labels, true_preds, average='macro')\n",
    "    report = classification_report(true_labels, true_preds)\n",
    "    \n",
    "    print(f\"Average Latency: {average_latency:.4f} seconds per batch\")\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start to run our code, starting by loading pre-trained model and tokenizer",
   "id": "204e35d7c39637d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:04.073559Z",
     "start_time": "2024-10-16T07:09:03.413613Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer, model = load_pretrained_ner_model()",
   "id": "8f205a52510775bd",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading the dataset",
   "id": "b885e56c70cdbd9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:04.109555Z",
     "start_time": "2024-10-16T07:09:04.082796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"data/DNRTI/test.txt\"\n",
    "\n",
    "sentences, labels = load_dataset(dataset_path)"
   ],
   "id": "9d2cb3706ce3b632",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Applying tag mapping to align dataset's labels with model's labels",
   "id": "664237f8d3555cbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:04.118632Z",
     "start_time": "2024-10-16T07:09:04.111754Z"
    }
   },
   "cell_type": "code",
   "source": "mapped_labels = map_tags(labels, tag_mapping, default_tag=\"O\")",
   "id": "dc819ce8cadf03c6",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the model's label list",
   "id": "e7df8a3d3f948284"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:04.135685Z",
     "start_time": "2024-10-16T07:09:04.129188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_labels = model.config.id2label\n",
    "label_list = [model_labels[i] for i in range(len(model_labels))]"
   ],
   "id": "4629b310726218af",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing the data",
   "id": "4639a51d15a747de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:04.216330Z",
     "start_time": "2024-10-16T07:09:04.137275Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_data = preprocess_data(tokenizer, sentences, mapped_labels, label_list)",
   "id": "47f512160bef3f4a",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And calling the evaluation function to print our metrics!",
   "id": "6fb8776a0ad3a924"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:09:23.693560Z",
     "start_time": "2024-10-16T07:09:04.217934Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_model(model, tokenized_data, label_list)",
   "id": "9cc90362f2ca801c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency: 0.9159 seconds per batch\n",
      "\n",
      "Evaluation Metrics:\n",
      "Precision: 0.7227\n",
      "Recall:    0.7058\n",
      "F1-Score:  0.7088\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACT       0.35      0.52      0.42       250\n",
      "         APT       0.79      0.58      0.67       369\n",
      "        FILE       0.92      0.70      0.79       248\n",
      "        IDTY       0.71      0.77      0.74       266\n",
      "         LOC       0.80      0.78      0.79       216\n",
      "         MAL       0.59      0.63      0.61       315\n",
      "     SECTEAM       0.84      0.89      0.87       152\n",
      "        TIME       0.81      0.79      0.80       169\n",
      "       VULID       0.69      0.70      0.70       132\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2117\n",
      "   macro avg       0.72      0.71      0.71      2117\n",
      "weighted avg       0.71      0.68      0.69      2117\n",
      "\n"
     ]
    }
   ],
   "execution_count": 98
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
